{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# After import make folders  following folders in sigTupl_data\n# 1. New_train_data , 2. New_train_data\n# upload Model_Weights_Best.hdf5 from computer located at :  E\\ modified Unet readimages_convert\\segmentation matlab and code paper with papers\\segmentation\n# rename Model_Weights_Best.hdf5 to Model_Weights.hdf5\n# now you can run the code\n\n#!git clone https://imran7778:7877787Awan@github.com/imran7778/segmentation_data.git\n#!git clone https://imran7778:7877787Awan@github.com/imran7778/segmentation_data.git\n!git clone https://imran7778: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@github.com/imran7778/segmentation_data.git","metadata":{"id":"mxPPnESkYzF7","outputId":"d75e7ac3-b9f8-4304-95c9-d4d4bbd04da7","execution":{"iopub.status.busy":"2023-03-30T13:03:39.858073Z","iopub.execute_input":"2023-03-30T13:03:39.858410Z","iopub.status.idle":"2023-03-30T13:03:48.296834Z","shell.execute_reply.started":"2023-03-30T13:03:39.858334Z","shell.execute_reply":"2023-03-30T13:03:48.295539Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'segmentation_data'...\nremote: Enumerating objects: 3585, done.\u001b[K\nremote: Counting objects: 100% (624/624), done.\u001b[K\nremote: Compressing objects: 100% (620/620), done.\u001b[K\nremote: Total 3585 (delta 4), reused 622 (delta 4), pack-reused 2961\u001b[K\nReceiving objects: 100% (3585/3585), 122.95 MiB | 31.55 MiB/s, done.\nResolving deltas: 100% (389/389), done.\nUpdating files: 100% (8053/8053), done.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Run only if you want to run unet++\n!pip install tensorflow-gpu==1.15.2\nimport tensorflow\nprint(tensorflow.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T14:11:51.335226Z","iopub.execute_input":"2021-11-15T14:11:51.335513Z","iopub.status.idle":"2021-11-15T14:12:46.014073Z","shell.execute_reply.started":"2021-11-15T14:11:51.335478Z","shell.execute_reply":"2021-11-15T14:12:46.011959Z"},"id":"uEmZ52ta6QOo"}},{"cell_type":"code","source":"# instruction to run\n#1. go to documents>Github>Segmentation\n#2. move the train folder (e.g SigTuple_data5) in Segmentation folder on which you want to train the #model and move other folders inside zewfolder\n# 3. open all_params3 in notepade and change paths according to folder\n#4. open GitHub desktop app> commit change> push origin\n# change %cd SigTuple_data5 in script where mikdir\n# chnage %cd SigTuple_data5 in scipt where ziping code\n# now run the code\n# at the end download zip file from Segmentation folder","metadata":{"id":"NmdKukTxErtj","execution":{"iopub.status.busy":"2023-03-30T13:03:48.299368Z","iopub.execute_input":"2023-03-30T13:03:48.299737Z","iopub.status.idle":"2023-03-30T13:03:48.305790Z","shell.execute_reply.started":"2023-03-30T13:03:48.299700Z","shell.execute_reply":"2023-03-30T13:03:48.304042Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# also change zip cell, also chnage all_params3\ni=3\n%cd segmentation_data\n%cd SigTuple_datac  \n!pwd\n!mkdir New_train_data\n!mkdir New_test_data\n!mkdir Submission_Data0\n!mkdir Submission_Data1\n!mkdir Submission_Data2\n!mkdir Submission_Data3\n!mkdir Submission_Data4\n!mkdir Submission_Data5\n!mkdir Submission_Data6\n!mkdir Submission_Data7\n\n#!mv Test_Data Test_Data-0\n#!mv Train_data Train_data-0\n%cd ..\n#!pwd","metadata":{"id":"chsBLbhbqsIs","outputId":"4911e9cb-7e26-44b8-85d5-cbd77520e69a","execution":{"iopub.status.busy":"2023-03-30T13:03:48.307970Z","iopub.execute_input":"2023-03-30T13:03:48.309273Z","iopub.status.idle":"2023-03-30T13:03:58.910582Z","shell.execute_reply.started":"2023-03-30T13:03:48.309217Z","shell.execute_reply":"2023-03-30T13:03:58.909305Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/segmentation_data\n/kaggle/working/segmentation_data/SigTuple_datac\n/kaggle/working/segmentation_data/SigTuple_datac\n/kaggle/working/segmentation_data\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install natsort\nFCN=0   # 1 for run, 0 for don't run that model \nexperi_repeat=100\nnum_epo=1500\nMC_UNet=0\nEffB0_drp=0\nSeg_model_with_drop=1\nEffB0_no_drp=0\nEffB7_drp=0\nEffB7_no_drp=0\nTRAIN_DATA_PATH = 'SigTuple_datac/Train_data/'\nNEW_TRAIN_DATA_PATH = 'SigTuple_datac/New_train_data/'\nNEW_TEST_DATA_PATH = 'SigTuple_datac/New_test_data/'\nTEST_DATA_PATH = 'SigTuple_datac/Test_Data/'\nSUBMISSION_DATA_PATH = 'SigTuple_datac/Submission_Data/'\nMODEL_CHECKPOINT_DIR = 'Checkpoints/'\nWEIGHTS = 'Model_Weights.hdf5'\nAUGMENT_TRAIN_DATA = False\nAUGMENT_TEST_DATA = False\nCREATE_EXTRA_DATA = False\nIMG_ROWS = 128\nIMG_COLS = 128\nIMG_START_NUM = 164\nSMOOTH = 1.0\nCLEAN_THRESH = 20\nTHRESH = 100\nBATCH_SIZE = 3\nEPOCHS = 50\nBASE_LR = 1e-04\nPATIENCE = 10\n\n# import train and test data paths from params\n%matplotlib inline\nimport os\nimport cv2\nimport natsort\nimport matplotlib.pyplot as plt\n#from all_params3 import TRAIN_DATA_PATH, TEST_DATA_PATH\n#from all_params3 import TRAIN_DATA_PATH, TEST_DATA_PATH\n\nimages_train = os.listdir(TRAIN_DATA_PATH)\nimages_test = os.listdir(TEST_DATA_PATH)\n\n# print number of train and test images\nprint ('number of train images: ', len(images_train)/2)\nprint ('number of train_mask images: ', len(images_train)/2)\nprint ('number of test images: ', len(images_test))\n\n##########   TRAINING Augmentation #################\nimport os\nimport cv2\nimport numpy as np\n#from all_params3 import *\n\n\nimage_names = os.listdir(TRAIN_DATA_PATH)\nimage_names.sort()\nimage_names=natsort.natsorted(image_names)\n\nimg_num = IMG_START_NUM\nprint('-'*30)\nprint('Creating new training images and store them in new_train_data_path...')\nprint('-'*30)\nfor img_name in image_names:\n    if 'mask' in img_name:\n        continue\n\n    img = cv2.imread(TRAIN_DATA_PATH + img_name)\n    \n    mask_img = cv2.imread(TRAIN_DATA_PATH + img_name.split('.')[0] + '-mask.jpg')\n    #import matplotlib.pyplot as plt\n    #import numpy as np\n    #plt.imshow(mask_img)\n    #sdvdv\n    if img.shape == (IMG_ROWS, IMG_COLS, 3):\n        cv2.imwrite(NEW_TRAIN_DATA_PATH + img_name, img)\n        cv2.imwrite(NEW_TRAIN_DATA_PATH + img_name.split('.')[0] + '-mask.jpg', mask_img)\n        continue\n    if CREATE_EXTRA_DATA == False:\n        continue\n\n    for row in range(0, img.shape[0], IMG_ROWS):\n        for col in range(0, img.shape[1], IMG_COLS):\n            new_img = img[row:row + IMG_ROWS, col:col + IMG_COLS, :]\n            new_mask_img = mask_img[row:row + IMG_ROWS, col:col + IMG_COLS, :]\n            if new_img.shape != (IMG_ROWS, IMG_COLS, 3) or np.max(new_mask_img) != 255.0:\n                continue\n            cv2.imwrite(NEW_TRAIN_DATA_PATH + 'train-' + str(img_num) + '.jpg', new_img)\n            cv2.imwrite(NEW_TRAIN_DATA_PATH + 'train-' + str(img_num) + '-mask.jpg', new_mask_img)\n            img_num = img_num + 1\nprint('successfully created.')\n\n##########    X-train Ytrain #################\nimport os\nimport cv2\nimport numpy as np\n#from all_params3 import * \n\npath=NEW_TRAIN_DATA_PATH\n#augment=AUGMENT_TRAIN_DATA\naugment=True\nimage_names = os.listdir(path)\nimage_names.sort()\nimage_names=natsort.natsorted(image_names)\n#image_names=image_names[1:]\n#print(image_names)\nimages_count = int(len(image_names) / 2)\nif augment == True:\n    images_count = images_count * 3\n#print(type(images_count))\nprint(images_count)\nprint(IMG_ROWS)\nprint(IMG_COLS)\nX_train = np.ndarray((images_count, 1, IMG_ROWS, IMG_COLS), dtype=np.uint8)\nY_train = np.ndarray((images_count, 1, IMG_ROWS, IMG_COLS), dtype=np.uint8)\n\ni = 0\nprint('-'*30)\nprint('Creating training images...')\nprint('-'*30)\nfor img_name in image_names:\n    if 'mask' in img_name:\n        continue\n    mask_img_name = img_name.split('.')[0] + '-mask.jpg'\n    img = cv2.imread(path + img_name, cv2.IMREAD_GRAYSCALE)\n    #print(type(CLEAN_THRESH))\n    #print(type(img))\n    img[img <= CLEAN_THRESH] = 255\n    mask_img = cv2.imread(path + mask_img_name, cv2.IMREAD_GRAYSCALE)\n    X_train[i] = np.array([img])\n    Y_train[i] = np.array([mask_img])\n    i = i + 1\n\n    if augment == True:\n        X_train[i] = np.array([img[:, ::-1]])\n        Y_train[i] = np.array([mask_img[:, ::-1]])\n        i = i + 1\n        X_train[i] = np.array([img[::-1, :]])\n        Y_train[i] = np.array([mask_img[::-1, :]])\n        i = i + 1\nprint('Loading done.')\nX_train = X_train.transpose((0, 2, 3, 1)) # change the shape\nY_train = Y_train.transpose((0, 2, 3, 1))\n\nX_train = X_train.astype('float32')\nY_train = Y_train.astype('float32')\nX_train /= 255.0\nY_train /= 255.0\n\n##########   TESTING AUGMENTATION #################\nimport os\nimport cv2\nimport numpy as np\n#from all_params3 import *\n\nimage_names = os.listdir(TEST_DATA_PATH )\nimage_names.sort()\nimage_names=natsort.natsorted(image_names)\nimg_num = IMG_START_NUM\nprint('-'*30)\nprint('Creating new testing images and store them in new_test_data_path...')\nprint('-'*30)\nfor img_name in image_names:\n    if 'mask' in img_name:\n        continue\n\n    \n    img = cv2.imread(TEST_DATA_PATH  + img_name)\n    mask_img = cv2.imread(TEST_DATA_PATH  + img_name.split('.')[0] + '-mask.jpg')\n    if img.shape == (IMG_ROWS, IMG_COLS, 3):\n        cv2.imwrite(NEW_TEST_DATA_PATH + img_name, img)\n        cv2.imwrite(NEW_TEST_DATA_PATH + img_name.split('.')[0] + '-mask.jpg', mask_img)\n        continue\n        sdvdsvdf\n        \n    if CREATE_EXTRA_DATA == False:\n        continue\n    \n    for row in range(0, img.shape[0], IMG_ROWS):\n        for col in range(0, img.shape[1], IMG_COLS):\n            sdbvdfvdfv\n            new_img = img[row:row + IMG_ROWS, col:col + IMG_COLS, :]\n            new_mask_img = mask_img[row:row + IMG_ROWS, col:col + IMG_COLS, :]\n            if new_img.shape != (IMG_ROWS, IMG_COLS, 3) or np.max(new_mask_img) != 255.0:\n                continue\n              \n            cv2.imwrite(NEW_TRAIN_DATA_PATH + 'train-' + str(img_num) + '.jpg', new_img)\n            cv2.imwrite(NEW_TRAIN_DATA_PATH + 'train-' + str(img_num) + '-mask.jpg', new_mask_img)\n            img_num = img_num + 1\nprint('successfully created.')\n\n##########    X-test Y-test #################\nX_test_names=[]\nimport os\nimport cv2\nimport numpy as np\n#from all_params3 import * \npath=NEW_TEST_DATA_PATH\naugment=False\nimage_names = os.listdir(path)\nimage_names.sort()\nimage_names=natsort.natsorted(image_names)\n#image_names=image_names[1:]\n#print(image_names)\nimages_count = int(len(image_names) / 2)\nif augment == True:\n    images_count = images_count * 3\n#print(type(images_count))\nprint(images_count)\nprint(IMG_ROWS)\nprint(IMG_COLS)\nX_test = np.ndarray((images_count, 1, IMG_ROWS, IMG_COLS), dtype=np.uint8)\nY_test = np.ndarray((images_count, 1, IMG_ROWS, IMG_COLS), dtype=np.uint8)\n\ni = 0\nprint('-'*30)\nprint('Creating testing images...')\nprint('-'*30)\nfor img_name in image_names:\n    if 'mask' in img_name:\n        continue\n    mask_img_name = img_name.split('.')[0] + '-mask.jpg'\n    img = cv2.imread(path + img_name, cv2.IMREAD_GRAYSCALE)\n    X_test_names.append(mask_img_name)\n    #print(type(CLEAN_THRESH))\n    #print(type(img))\n    img[img <= CLEAN_THRESH] = 255\n    mask_img = cv2.imread(path + mask_img_name, cv2.IMREAD_GRAYSCALE)\n    \n    X_test[i] = np.array([img])\n    Y_test[i] = np.array([mask_img])\n    i = i + 1\n\n    if augment == True:\n        X_test[i] = np.array([img[:, ::-1]])\n        Y_test[i] = np.array([mask_img[:, ::-1]])\n        i = i + 1\n        X_test[i] = np.array([img[::-1, :]])\n        Y_test[i] = np.array([mask_img[::-1, :]])\n        i = i + 1\nprint('Loading done.')\nX_test = X_test.transpose((0, 2, 3, 1)) # change the shape\nY_test = Y_test.transpose((0, 2, 3, 1))\n\nX_test1=X_test\nY_test1=Y_test\nX_test = X_test.astype('float32')\nY_test = Y_test.astype('float32')\nX_test /= 255.0\nY_test /= 255.0","metadata":{"id":"EpUUl2SUrSdU","outputId":"1e1cf99e-e2b7-4800-98da-9645af365a03","execution":{"iopub.status.busy":"2023-03-30T13:03:58.914904Z","iopub.execute_input":"2023-03-30T13:03:58.915196Z","iopub.status.idle":"2023-03-30T13:04:09.932153Z","shell.execute_reply.started":"2023-03-30T13:03:58.915167Z","shell.execute_reply":"2023-03-30T13:04:09.929525Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting natsort\n  Downloading natsort-8.3.1-py3-none-any.whl (38 kB)\nInstalling collected packages: natsort\nSuccessfully installed natsort-8.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mnumber of train images:  240.0\nnumber of train_mask images:  240.0\nnumber of test images:  120\n------------------------------\nCreating new training images and store them in new_train_data_path...\n------------------------------\nsuccessfully created.\n720\n128\n128\n------------------------------\nCreating training images...\n------------------------------\nLoading done.\n------------------------------\nCreating new testing images and store them in new_test_data_path...\n------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/2046959074.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIMG_ROWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_COLS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNEW_TEST_DATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNEW_TEST_DATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-mask.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0msdvdsvdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgcodecs/src/loadsave.cpp:799: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n"],"ename":"error","evalue":"OpenCV(4.5.4) /tmp/pip-req-build-jpmv6t9_/opencv/modules/imgcodecs/src/loadsave.cpp:799: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n","output_type":"error"}]},{"cell_type":"code","source":"\n# Original dice for 1 channel \nimport numpy as np\nfrom tensorflow.keras import backend as K\nfrom all_params3 import SMOOTH\n\ndef dice_coef(y_true, y_pred):\n    y_true_flat = K.flatten(y_true)\n    y_pred_flat = K.flatten(y_pred)\n    intersection = K.sum(y_true_flat * y_pred_flat)\n    return (2.0 * intersection + SMOOTH) / (K.sum(y_true_flat) + K.sum(y_pred_flat) + SMOOTH)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)\n\ndef np_dice_coef(y_true, y_pred):\n    y_true_flat = y_true.flat[:]\n    y_pred_flat = y_pred.flat[:]\n    intersection = np.sum(y_true_flat * y_pred_flat)\n    return (2.0 * intersection + SMOOTH) / (np.sum(y_true_flat) + np.sum(y_pred_flat) + SMOOTH)\n\ndef np_dice_coef_loss(y_true, y_pred):\n    return 1.0 - np_dice_coef(y_true, y_pred)","metadata":{"id":"LRKp1FA_EruI","execution":{"iopub.status.busy":"2023-03-30T13:04:09.933292Z","iopub.status.idle":"2023-03-30T13:04:09.933989Z","shell.execute_reply.started":"2023-03-30T13:04:09.933734Z","shell.execute_reply":"2023-03-30T13:04:09.933761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras import backend as K\n\n'''\nEVALUATION METRICS using y_true and y_pred\n\n- Dice coefficient\n- Intersection Over Union (IOU)\n- Precision\n- Recall\n'''\n\ndef dice_coefficient(y_true, y_pred):\n    # flatten the image arrays for true and pred\n    y_true=K.flatten(y_true)\n    y_pred=K.flatten(y_pred[:,:,:,0])\n\n    epsilon=1.0 # to prevent dividing by zero\n    return (2*K.sum(y_true*y_pred)+epsilon)/(K.sum(y_true)+K.sum(y_pred)+epsilon)\n\ndef dice_loss(y_true, y_pred):\n    return 1-dice_coefficient(y_true, y_pred)\n\ndef recall(y_true, y_pred):\n    # flatten the image arrays for true and pred\n    y_pred = K.flatten(y_pred)\n    y_true = K.flatten(y_true)\n    return (K.sum(y_true * y_pred)/ (K.sum(y_true) + K.epsilon()))  \n\ndef np_recall(y_true, y_pred):\n    y_true = y_true.flat[:]\n    y_pred = y_pred.flat[:]\n    return (np.sum(y_true * y_pred)/ (np.sum(y_true) + K.epsilon()))  \n\ndef precision(y_true, y_pred):\n    y_pred = K.flatten(y_pred)\n    y_true = K.flatten(y_true)\n    return (K.sum(y_true * y_pred) / (K.sum(y_pred) + K.epsilon()))  \n\ndef np_precision(y_true, y_pred):\n    y_true = y_true.flat[:]\n    y_pred = y_pred.flat[:]\n    return (np.sum(y_true * y_pred) / (np.sum(y_pred) + K.epsilon()))  \n\ndef iou(y_true, y_pred):  #this can be used as a loss if you make it negative\n    y_pred = K.flatten(y_pred)\n    y_true = K.flatten(y_true)\n    union = y_true + ((1 - y_true) * y_pred)\n    return (K.sum(y_true * y_pred) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n\ndef np_iou(y_true, y_pred):\n    y_true = y_true.flat[:]\n    y_pred = y_pred.flat[:]\n    union = y_true + ((1 - y_true) * y_pred)\n    return (np.sum(y_true * y_pred) + K.epsilon()) / (np.sum(union, axis=-1) + K.epsilon())\n\n\ndef iou_loss(y_true, y_pred, smooth=1):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n    iou = (intersection + smooth) / ( union + smooth)\n    return iou\n\ndef FPR(y_true, y_pred):\n    y_true=-1*y_true\n    y_true_flat = y_true.flat[:]\n    y_pred_flat = y_pred.flat[:]\n    intersection = np.sum(y_true_flat * y_pred_flat)\n    return (intersection), (np.sum(y_true_flat))","metadata":{"id":"8vjZHEK7EruM","execution":{"iopub.status.busy":"2023-03-30T13:04:09.936111Z","iopub.status.idle":"2023-03-30T13:04:09.936601Z","shell.execute_reply.started":"2023-03-30T13:04:09.936352Z","shell.execute_reply":"2023-03-30T13:04:09.936376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Entropy(X, axis=-1):\n    '''\n    Helper function to compute entropy: all uncertainty metrics computed in calc_Uncertainty()\n    '''\n\n    return -1* np.sum(X * np.log(X+1E-12), axis=axis)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.938279Z","iopub.status.idle":"2023-03-30T13:04:09.938769Z","shell.execute_reply.started":"2023-03-30T13:04:09.938502Z","shell.execute_reply":"2023-03-30T13:04:09.938524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dice_coef_loss is better than binary_crossentropy, bce_jaccard_loss\n# run all if you stop the training, don't just run this cell because you will get bad val_loss \n!pip install git+https://github.com/imran7778/segmentation_models.git\n#from tensorflow.keras.optimizers import Adam, SGD\nfrom segmentation_models import Unet\nfrom segmentation_models import get_preprocessing\nfrom segmentation_models.losses import bce_jaccard_loss\nfrom segmentation_models.metrics import iou_score\nimport tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam, SGD\n\n\n\nimport os\nimport cv2\nimport numpy as np\nfrom skimage.measure import label\nfrom skimage.measure import regionprops\nimage_names = os.listdir('SigTuple_datac/Test_Data/')\nimage_names.sort()\nimage_names=natsort.natsorted(image_names)\n\n#BACKBONE = 'efficientnetb0' 'efficientnetb1' 'efficientnetb2' \n#'resnet18' 'resnet34' 'resnet50' 'resnet101' 'resnet152'\n#'efficientnetb3' 'efficientnetb4' 'efficientnetb5'  'efficientnetb7'\n#'densenet121' 'densenet169' 'densenet201'\nlosses=[]\nall_score=[]\nTP=tf.keras.metrics.TruePositives()\nTN=tf.keras.metrics.TrueNegatives()\nFP=tf.keras.metrics.FalsePositives()\nFN=tf.keras.metrics.FalseNegatives()\nBACKBONE = ['efficientnetb0','efficientnetb1', 'efficientnetb2','efficientnetb3',\n            'efficientnetb4', 'efficientnetb5','efficientnetb6', 'efficientnetb7',\n            'resnet18','resnet34','resnet50','resnet101','resnet152']\nprint(BACKBONE)\n#BACKBONE='efficientnetb1'\nopt = Adam(learning_rate=1e-2)\nloss_func='binary_crossentropy' #dice_loss, dice_coef_loss\nmetrics = [dice_coef,iou,precision,recall,FP,FN,TP,TN]\nN=100\nselect=0\nfor i in range(N):\n      if i==select:\n        #preprocess_input = get_preprocessing(BACKBONE) # you cannot pre-process if input is (128,128,1)\n        #X_train = preprocess_input(X_train)\n        #X_test = preprocess_input(X_test)\n        #preprocess_input = get_preprocessing(BACKBONE)\n        print('================================='+BACKBONE[i]+'========================================')\n        print(BACKBONE[i])\n        model = Unet(BACKBONE[i], input_shape=(128, 128, 1), classes=1, encoder_weights=None)\n        model.summary()\n        sdvsdv\n        model.compile(optimizer=opt, loss=dice_coef_loss, metrics=metrics)\n        callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, min_lr=1e-07, min_delta=0.001, verbose=1),\n                    EarlyStopping(monitor=\"val_loss\",min_delta=0,patience=100,verbose=1,mode=\"min\",restore_best_weights=True)]\n        loss=model.fit(X_train, Y_train, batch_size=32, epochs=num_epo, callbacks=callbacks, validation_split=0.1, shuffle=True)\n        losses.append(loss)\n        model.save_weights(WEIGHTS)\n        #################################################################################\n\n        #score = model.evaluate(X_test, Y_test, verbose=1)\n        #all_score.append(score)\n        #print(model.metrics_names)\n        #print('Metrics:', score)\n        #################################################################################\n        #SUBMISSION_DATA_PATH = 'SigTuple_datac/Submission_Data'+str(i)+'/'\n        #model.load_weights(WEIGHTS)\n\n        #for i in range(0,len(X_test)):\n            #test_img1=X_test[i]\n            #test_img1 = np.array([test_img1], dtype=np.float32)\n            #c=0\n            #r=0\n            #test_mask_img = model.predict(test_img1, verbose=1)\n            ##test_mask_img = model.predict(test_img.transpose(0, 2, 3, 1), verbose=1)\n            #test_mask_img = (test_mask_img * 255.0).astype(np.uint8)\n            #test_mask_img = test_mask_img.transpose(0, 3, 1, 2)[0][0]\n            #mask_img_name=X_test_names[i]\n            #mask_img[r:r + IMG_ROWS, c:c + IMG_COLS] = test_mask_img\n            #cv2.imwrite(SUBMISSION_DATA_PATH + mask_img_name, mask_img)\n\n","metadata":{"id":"d2P_MNT4EruO","outputId":"f8affa25-6485-466a-ea13-f9155f9afec4","execution":{"iopub.status.busy":"2023-03-30T13:04:09.941739Z","iopub.status.idle":"2023-03-30T13:04:09.942212Z","shell.execute_reply.started":"2023-03-30T13:04:09.941968Z","shell.execute_reply":"2023-03-30T13:04:09.941990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Img_num=0\nprint('Original image')\ntest_img1=X_test[Img_num]\nplt.figure(1)\nplt.imshow(np.squeeze(test_img1))\n\nprint('GT image')\ntest_img1=Y_test[Img_num]\nplt.figure(2)\nplt.imshow(np.squeeze(test_img1))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.943882Z","iopub.status.idle":"2023-03-30T13:04:09.944379Z","shell.execute_reply.started":"2023-03-30T13:04:09.944108Z","shell.execute_reply":"2023-03-30T13:04:09.944131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Predicted image')\ntest_img1=X_test[Img_num]\ntest_img1 = np.array([test_img1], dtype=np.float32)\ntest_img1=model.predict(test_img1)\nplt.figure(3)\nplt.imshow(np.squeeze(test_img1))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.945988Z","iopub.status.idle":"2023-03-30T13:04:09.946693Z","shell.execute_reply.started":"2023-03-30T13:04:09.946432Z","shell.execute_reply":"2023-03-30T13:04:09.946455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(WEIGHTS)\nfor i in range(0,len(X_test)):\n    SUBMISSION_DATA_PATH = 'SigTuple_datac/Submission_Data'+str(0)+'/'\n    image_names = os.listdir('SigTuple_datac/Test_Data/')\n    image_names.sort()\n    image_names=natsort.natsorted(image_names)\n    N=experi_repeat\n    test_img1=X_test[i]\n    #test_mask_img = (test_img1 * 255.0).astype(np.uint8)\n    #plt.figure(i)\n    #plt.imshow(test_mask_img)\n    test_img1 = np.array([test_img1], dtype=np.float32)\n    c=0\n    r=0\n    for n in range(N):\n        test_mask_img=model.predict(test_img1, verbose=1)\n        test_mask_img = (test_mask_img * 255.0).astype(np.uint8)\n        test_mask_img = test_mask_img.transpose(0, 3, 1, 2)[0][0]\n        mask_img_name=str(i)+'-'+str(n)+'.jpg' # i represent image numbr, n respresent experiment numbr\n        mask_img[r:r + IMG_ROWS, c:c + IMG_COLS] = test_mask_img\n        cv2.imwrite(SUBMISSION_DATA_PATH + mask_img_name, mask_img)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.948486Z","iopub.status.idle":"2023-03-30T13:04:09.948979Z","shell.execute_reply.started":"2023-03-30T13:04:09.948731Z","shell.execute_reply":"2023-03-30T13:04:09.948755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd SigTuple_datac \n!zip -r Submission_Data0.zip  Submission_Data0\n\n%mv Submission_Data0.zip ../\n\n%cd ..\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.950608Z","iopub.status.idle":"2023-03-30T13:04:09.951091Z","shell.execute_reply.started":"2023-03-30T13:04:09.950846Z","shell.execute_reply":"2023-03-30T13:04:09.950868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate uncertainty metrics\ndef calc_Uncertainty(preds):\n    # calculate mean pred=(10,128, 128,1), mean_preds=(128,128,1). So np.mean is taking averge of all 10 image and make a new average value image of size (128,128,1)\n    #print(\"preds=\",preds.shape)\n    mean_preds = np.mean(preds, axis=0) #axis=0 mean columns wise, axis=1 mean rows wise\n    #print(\"mean_preds=\",mean_preds.shape)\n\n\n############################## shape (128,128) is same as (128,128,1) there is no difference both are images   #################################\n\n    # calculate entropy\n    entropy=Entropy(np.mean(preds, axis=0),axis=-1) # shape= (128, 128), take mean image of all 10 image and then their entropy is calculated by above formula \n    #print(\"entropy=\",entropy.shape)\n    # Expected entropy of the predictive under the parameter posterior\n    entropy_exp = np.mean(Entropy(preds, axis=1)) #shape=() mean a scalar value\n    #print(\"entropy_exp=\",entropy_exp.shape)\n    \n    # calculate mutual info\n    mutual_info = entropy - entropy_exp  # Equation 2 of https://arxiv.org/pdf/1711.08244.pdf\n    #print(\"mutual_info=\",mutual_info.shape) # shape= (128, 128), \n   \n    # calculate variance\n    #print(preds[:])\n    variance = np.std(preds[:], 0) # shape= (128, 128,1), take std of pixels (each pixel with coressponding 10 images), and make a new image of std value image made up of 10 images\n    #print(\"variance=\",variance.shape)  \n \n    # calculate aleatoric uncertainty\n    aleatoric = np.mean(preds*(1-preds), axis=0) # shape= (128, 128)\n    # calculate epistemic uncertainty\n    epistemic = np.mean(preds**2, axis=0) - np.mean(preds, axis=0)**2\n    # overall alertoric + epistemic\n    overall= aleatoric + epistemic\n    return mean_preds, entropy,mutual_info, variance, aleatoric, epistemic , overall","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.952841Z","iopub.status.idle":"2023-03-30T13:04:09.953334Z","shell.execute_reply.started":"2023-03-30T13:04:09.953075Z","shell.execute_reply":"2023-03-30T13:04:09.953096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a = np.array([[1, 2, 3, 5 ,8 ,7]])\n#print(a)\n#print(a.shape)\n#b=np.mean(a, axis=1)\n#b=np.squeeze(a)\n#print(b)\n#print(b.shape)\n#dsvd\nfrom keras import metrics as metrics\nmodel.load_weights(WEIGHTS)\ndef inference(model, image, y_true, N=10, tau=0.01):\n    loss_l=[]\n    dice_l=[]\n    iou_l=[]\n    prec_l=[]\n    recal_l=[]\n    FPR_l=[]\n    FNR_l=[]\n    \n\n    # add batch dimension\n    image = np.expand_dims(image, 0)\n    y_true = np.expand_dims(y_true , 0)\n    \n    # perform N stochastic forward passes and then append the preds\n    preds = []\n    for n in range(N):\n        pred=model.predict(image)\n        preds.append(pred)\n    preds = np.array(preds)  # (N,128,128)\n\n\n    # calculate the uncertainty metrics\n    prediction, entropy, mutual_info, variance, aleatoric,epistemic, overall =  calc_Uncertainty(preds) # all metric have shape (128,128,1)\n    \n    # calculate the accuracy metrics\n    #loss, bin_acc, dice, precision, recall, iou,iou_loss  = model.evaluate(image, y_true, batch_size=1)\n    for n in range(N):\n        loss,dice_coefficient,iou,precision,recall,FP,FN,TP,TN = model.evaluate(image, y_true, batch_size=1)\n        loss_l.append(loss)\n        dice_l.append(dice_coefficient)\n        iou_l.append(iou)\n        prec_l.append(precision)\n        recal_l.append(recall)\n        FPR = FP/(FP+TN)\n        FNR = FN/(FN+TP)\n        FPR_l.append(FPR)\n        FNR_l.append(FNR)\n        \n  \n    #print(loss_l)\n    m_loss=np.mean(loss_l)\n    m_dice=np.mean(dice_l)\n    m_iou=np.mean(iou_l)\n    m_prec=np.mean(prec_l)\n    m_recal=np.mean(recal_l)\n    m_FPR=np.mean(FPR_l)\n    m_FNR=np.mean(FNR_l)\n\n    \n    std_loss=np.std(loss_l)\n    std_dice=np.std(dice_l)\n    std_iou=np.std(iou_l)\n    std_prec=np.std(prec_l)\n    std_recal=np.std(recal_l)\n    std_FPR=np.std(FPR_l)\n    std_FNR=np.std(FNR_l)\n    #print(m_loss)\n    #print(std_loss)\n    #sdvsdv\n    #m_dice=np.mean(dice_l)\n    #m_iou=np.mean(iou_l)\n\n    error=y_true[0]-prediction # shape (128,128,1)\n    \n    # THRESHOLDING BASED ON UNCERTAINTY \n    \n    #############################################################################\n    #\n    # This will be used to flag cases to a doctor for referral\n    #\n    ##############################################################################\n    \n    #prediction = np.where(overall > tau*np.max(overall), 1, 0)\n    \n    return  np.squeeze(prediction), np.squeeze(aleatoric), np.squeeze(epistemic),np.squeeze(entropy), np.squeeze(mutual_info),np.squeeze(variance), np.squeeze(error), (m_loss,m_dice, m_iou,m_prec,m_recal,m_FPR,m_FNR),(std_loss,std_dice,std_iou,std_prec,std_recal,std_FPR,std_FNR)#(loss_l,dice_l,iou,prec_l,recal_l,FP_l,FN_l, TP_l,TN_l)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.956784Z","iopub.status.idle":"2023-03-30T13:04:09.957271Z","shell.execute_reply.started":"2023-03-30T13:04:09.957009Z","shell.execute_reply":"2023-03-30T13:04:09.957032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nnum = len(X_test)\nMC_stochastic=[experi_repeat]\nuncertainty = {}\nname_l = {}\nimage_set=list(range(0, num))\nfor idx, samples in enumerate(MC_stochastic):\n    start = time.time()\n    # UNCERTAINTY METRICS\n\n\n\n\n\n    \n    # for MC_stochastic=3 inference will run 3 times(3 forward pass) and we get \n    #3 values for each (prediction, aleatoric, epistemic, entropy, mi, variance, error,scores). then we append \n    #the MEAN value of the 3 results.then for next image X_test[i] 3 forwards pass\n    #then for next image X_test[i] 3 forward pass untill all , similarly for next all test images. Then MC_stochastic=5 and repeat the process and so on.\n    for indx,i in enumerate(image_set):\n        image = X_test[i]\n        gt    = Y_test[i]\n        print('#########uncertainty for image=' + str(i)+'##############')\n        #plt.figure(i)\n        #plt.imshow(np.squeeze(image))\n        #plt.xlabel('image_'+ str(i))\n        \n        \n        \n        \n        #######################################\n        prediction, aleatoric, epistemic, entropy, mi, variance, error,m_scores, std_scores = inference(model, image, gt, N=samples, tau=0.01) # these all metrics are images (128,128) except m_score and std_score\n        gt=np.reshape(gt,(128, 128))\n        error=abs(error)\n\n            \n        #loss,dice_coefficient,iou,precision,recall,FPR,FNR =score\n        \n        end = time.time()\n        #name_l=['mean_aleatoric', 'std_aleatoric', 'm_episte', 'std_episte', 'm_entpy', 'std_entpy']\n\n        #here we are coverting the image into mean and std values\n\n        name_l.update({ '{}'.format(str(i)) :  # put all results values in uncertainty cell array\n        ['m_alearic_img_'+str(i), 'std_alea_img_'+str(i),\n          'm_epist_img_'+str(i), 'std_episte_img_'+str(i),\n          'm_entpy_imge_'+str(i), 'std_entpy_img_'+str(i),\n          'm_mi_imge_'+str(i), 'std_mi_img_'+str(i),\n          'm_var_imge_'+str(i), 'std_var_img_'+str(i),  # here taking mean and std of variance which is acctually an image. so converting the image into a value\n          'm_error_imge_'+str(i), 'std_error_img_'+str(i),\n          'm_loss_imge_'+str(i), 'std_loss_img_'+str(i),  # here i am not taking mean and std, but its acctually accessing above value\n          'm_dice_imge_'+str(i), 'std_dice_img_'+str(i),\n          'm_iou_imge_'+str(i), 'std_iou_img_'+str(i),\n          'm_prec_imge_'+str(i), 'std_prec_img_'+str(i),\n          'm_recal_imge_'+str(i), 'std_recal_img_'+str(i),\n          'm_FPR_imge_'+str(i), 'std_FPR_img_'+str(i),\n          'm_FNR_imge_'+str(i), 'std_FNR_img_'+str(i),\n        ]} )\n\n        uncertainty.update({ '{}'.format(str(i)) :  # put all results values in uncertainty cell array\n        [np.mean(aleatoric), np.std(aleatoric),\n          np.mean(epistemic), np.std(epistemic),\n          np.mean(entropy), np.std(entropy),\n          np.mean(mi), np.std(mi),\n          np.mean(variance), np.std(variance),\n          np.mean(error), np.std(error),\n          m_scores[1], std_scores[1],\n          m_scores[2], std_scores[2],\n          m_scores[3], std_scores[3],\n          m_scores[4], std_scores[4],\n          m_scores[5], std_scores[5],\n          m_scores[6], std_scores[6],\n          np.mean(prediction), np.std(prediction),\n        ]} )\n        \n\n    \n    print(name_l)\n    print(uncertainty)\n    x=len(uncertainty[str(3)][:]) # str(n) where n is first element of MC_stochastics i.e 3 or may be 100\n    \n    rresults = np.zeros((x, len(image_set)))\n    for indx,i  in enumerate(image_set):\n      rresults[:, indx] = uncertainty[str(i)][:]\n    print(rresults[:, indx])\n\n\nimport csv\nif MC_UNet==1:\n    name_file='Results_MC_UNet.csv'\nelif EffB0_drp==1:\n    name_file='Results_B0_drp.csv'\nelif Seg_model_with_drop==1:\n    name_file='Seg_drp.csv'\nwith open(name_file, 'w') as f:\n    writer = csv.writer(f)\n\n    # write the header\n    writer.writerow(rresults)\n\nf.close()\nresults=1\n\n#in csv file, to view all values you have to copy paste each cell in word, Note: don't double click on cell in excel to view all values, copy paste it without double click\n#first cell respresent m_alearic values of all tested image, second cell resprest std_alea value of all tested image. and so on the sequece is shown below line\n#m_alearic_img', 'std_alea_img', 'm_epist_img', 'std_episte_img', 'm_entpy_imge', 'std_entpy_img', 'm_mi_imge', 'std_mi_img', 'm_var_imge', 'std_var_img', 'm_error_imge', 'std_error_img', 'm_loss_imge', 'std_loss_img', 'm_dice_imge', 'std_dice_img', 'm_iou_imge', 'std_iou_img, 'm_prec_imge', 'std_prec_img', 'm_recal_imge', 'std_recal_img', 'm_FPR_imge', 'std_FPR_img', 'm_FNR_imge', 'std_FNR_img'","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.959019Z","iopub.status.idle":"2023-03-30T13:04:09.959505Z","shell.execute_reply.started":"2023-03-30T13:04:09.959262Z","shell.execute_reply":"2023-03-30T13:04:09.959286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"expenses =  rresults\n!pip install openpyxl\nimport openpyxl\n\nwb = openpyxl.Workbook()\n\nsheet = wb.active\n\nle_ = len(expenses)\np = 0\n\nfor i in expenses:\n     k = 0\n     for j in i:\n          c1 = sheet.cell(row=p+1,column=k+1)\n          c1.value = str(j)\n          k+=1\n     p+=1\n\nwb.save(\"demo1.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.961069Z","iopub.status.idle":"2023-03-30T13:04:09.962105Z","shell.execute_reply.started":"2023-03-30T13:04:09.961859Z","shell.execute_reply":"2023-03-30T13:04:09.961883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results=rresults;\nplt.rcParams.update({'font.size': 22})\nf=28\nfig, ax = plt.subplots(nrows=4, ncols=3, figsize=(38,20), sharex=True)\nax[0,0].grid()\nax[0,1].grid()\nax[0,2].grid()\nax[1,0].grid()\nax[1,1].grid()\nax[1,2].grid()\nax[2,0].grid()\nax[2,1].grid()\nax[2,2].grid()\nax[3,0].grid()\nax[3,1].grid()\nax[3,2].grid()\n\nax[0,0].errorbar(image_set,results[0], yerr=results[1]/np.sqrt(MC_stochastic))\nax[0,0].set_ylabel('Aleatoric uncertainty')\nax[0,1].errorbar(image_set,results[2], yerr=results[3]/np.sqrt(MC_stochastic))\nax[0,1].set_ylabel('Epistemic uncertainty')\nax[0,2].errorbar(image_set,results[4], yerr=results[5]/np.sqrt(MC_stochastic))\nax[0,2].set_ylabel('Entropy')\n#ax[1,0].errorbar(MC_stochastic,results[6], yerr=results[7]/np.sqrt(MC_stochastic))\n#ax[1,0].set_ylabel('MI')\nax[1,0].errorbar(image_set,results[8], yerr=results[9]/np.sqrt(MC_stochastic))\nax[1,0].set_ylabel('Variance',fontsize=f)\nax[1,1].errorbar(image_set,results[10], yerr=results[11]/np.sqrt(MC_stochastic))\nax[1,1].set_ylabel('Error')\nax[1,2].errorbar(image_set,results[12], yerr=results[13])\nax[1,2].set_ylabel('Dice Coff')\n\nax[2,0].errorbar(image_set,results[14], yerr=results[15])\nax[2,0].set_ylabel('IOU')\nax[2,1].errorbar(image_set,results[16], yerr=results[17])\nax[2,1].set_ylabel('Precision')\nax[2,2].errorbar(image_set,results[18], yerr=results[19])\nax[2,2].set_ylabel('Recall')\n\nax[3,0].errorbar(image_set,results[20], yerr=results[21])\nax[3,0].set_ylabel('FPR')\nax[3,1].errorbar(image_set,results[22], yerr=results[23])\nax[3,1].set_ylabel('FNR')\nax[3,2].errorbar(image_set,results[24], yerr=results[25]/np.sqrt(MC_stochastic),linewidth=3,capsize=10,fmt='-.o')\nax[3,2].set_ylabel('Prediction')\n\n\n\nax[0,0].set_xlabel('(a) Test Image Numbering')\nax[0,1].set_xlabel('(b) Test Image Numbering')\nax[0,2].set_xlabel('(c) Test Image Numbering')\nax[1,0].set_xlabel('(d) Test Image Numbering')\nax[1,1].set_xlabel('(e) Test Image Numbering')\nax[1,2].set_xlabel('(f) Test Image Numbering')\nax[2,0].set_xlabel('(d) Test Image Numbering')\nax[2,1].set_xlabel('(e) Test Image Numbering')\nax[2,2].set_xlabel('(f) Test Image Numbering')\nax[3,0].set_xlabel('(g) Test Image Numbering')\nax[3,1].set_xlabel('(h) Test Image Numbering')\nax[3,2].set_xlabel('(i) Test Image Numbering')\n\nax[3,2].set_title('Total number of repeated experiments: 100')\n\n#plt.savefig('errorbars.png',dpi=500)\n\n#############################################################################################\n#https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html  \n\n\nplt.rcParams.update({'font.size': 22})\nf=28\nfig, ax = plt.subplots(nrows=4, ncols=3, figsize=(38,20), sharex=True)\nax[0,0].grid()\nax[0,1].grid()\nax[0,2].grid()\nax[1,0].grid()\nax[1,1].grid()\nax[1,2].grid()\nax[2,0].grid()\nax[2,1].grid()\nax[2,2].grid()\nax[3,0].grid()\nax[3,1].grid()\nax[3,2].grid()\n\nax[0,0].plot(image_set,results[1],'o-r',linewidth=2, label='line 1')\n#ax[0,0].plot(image_set,results[3],'o-g',linewidth=2, label='line 2')\nax[0,0].legend()\nax[0,0].set_ylabel('Aleatoric uncertainty')\nax[0,1].plot(image_set,results[3],'o-r',linewidth=2, label='line 1')\nax[0,1].legend()\nax[0,1].set_ylabel('Epistemic uncertainty')\nax[0,2].plot(image_set,results[4],'o-r',linewidth=2, label='line 1')\nax[0,2].legend()\nax[0,2].set_ylabel('Entropy')\n#ax[1,0].errorbar(MC_stochastic,results[6], yerr=results[7]/np.sqrt(MC_stochastic))\n#ax[1,0].set_ylabel('MI')\nax[1,0].plot(image_set,results[9],'o-r',linewidth=2, label='line 1')\nax[1,0].legend()\nax[1,0].set_ylabel('Variance',fontsize=f)\nax[1,1].plot(image_set,results[11],'o-r',linewidth=2, label='line 1')\nax[1,1].legend()\nax[1,1].set_ylabel('Error')\nax[1,2].plot(image_set,results[13],'o-r',linewidth=2, label='line 1')\nax[1,2].legend()\nax[1,2].set_ylabel('Dice Coff')\n\nax[2,0].plot(image_set,results[15],'o-r',linewidth=2, label='line 1')\nax[2,0].legend()\nax[2,0].set_ylabel('IOU')\nax[2,1].plot(image_set,results[17],'o-r',linewidth=2, label='line 1')\nax[2,1].legend()\nax[2,1].set_ylabel('Precision')\nax[2,2].plot(image_set,results[19],'o-r',linewidth=2, label='line 1')\nax[2,2].legend()\nax[2,2].set_ylabel('Recall')\n\nax[3,0].plot(image_set,results[21],'o-r',linewidth=2, label='line 1')\nax[3,0].legend()\nax[3,0].set_ylabel('FPR (Up)')\nax[3,1].plot(image_set,results[23],'o-r',linewidth=2, label='line 1')\nax[3,1].legend()\nax[3,1].set_ylabel('FNR (Qp)')\nax[3,2].plot(image_set,results[25],'o-r',linewidth=2, label='line 1')\nax[3,2].legend()\nax[3,2].set_ylabel('Prediction')\n\n\n\nax[0,0].set_xlabel('(a) Test Image Numbering')\nax[0,1].set_xlabel('(b) Test Image Numbering')\nax[0,2].set_xlabel('(c) Test Image Numbering')\nax[1,0].set_xlabel('(d) Test Image Numbering')\nax[1,1].set_xlabel('(e) Test Image Numbering')\nax[1,2].set_xlabel('(f) Test Image Numbering')\nax[2,0].set_xlabel('(d) Test Image Numbering')\nax[2,1].set_xlabel('(e) Test Image Numbering')\nax[2,2].set_xlabel('(f) Test Image Numbering')\nax[3,0].set_xlabel('(g) Test Image Numbering')\nax[3,1].set_xlabel('(h) Test Image Numbering')\nax[3,2].set_xlabel('(i) Test Image Numbering')\n\nax[3,2].set_title('Total number of repeated experiments: 100')\n\n#plt.savefig('Uncertainty.png',dpi=500)\n#print(prec_list)\n#print(FNR1_list)\n#print(FPR1_list)\n  #aleatoric=[x,x,x] for MC_stochastic=3\n  # aleatoric_list[mean.aleatoric for X_test[1] , mean.aleatoric for X_test[2].... , mean.aleatoric for X_test[i]]\n  #uncertainty={[mean.aleatoric_list, std.aleatoric_list (for MC_stochastic=3)],[mean.aleatoric_list,std.aleatoric_list (for MC_stochastic=5)], []....[]}","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.963539Z","iopub.status.idle":"2023-03-30T13:04:09.964023Z","shell.execute_reply.started":"2023-03-30T13:04:09.963774Z","shell.execute_reply":"2023-03-30T13:04:09.963796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_predict=X_test\nY_predict=Y_test\nnum = len(X_predict)\nplt.rcParams['image.cmap'] = 'viridis'\nimport matplotlib.gridspec as gridspec\nfor i in range(num):\n    #sample = np.random.randint(0,len(X_predict))\n    sample=i\n    image = X_predict[sample]\n    true  = Y_predict[sample]\n    #image = np.expand_dims(image, 0)\n    \n    #model.load_weights(WEIGHTS)\n    #plt.imshow(np.squeeze(image))\n    #plt.show()\n    \n    #preds = []\n    #for n in range(10):\n        #pred=model.predict(image)\n        #preds.append(pred)\n    #preds = np.array(preds)\n    \n\n    #for n in range(10):\n    #  plt.figure(n+1)\n     # plt.imshow(np.squeeze(preds[n]))\n      \n    #mean_preds = np.mean(preds, axis=0)\n    #plt.figure(20)\n    #plt.imshow(np.squeeze(mean_preds))\n    #gbrtgrtg\n\n    \n    #Pass image (128,128,3) to inference, where this image is predicts 10 times using model.predict(with dropout to ensure practicle senario) to get preds (10,128,128,1)\n    #prediction, aleatoric, epistemic, entropy, mi, variance, error, scores = inference(model, image, true , N=10, tau=0.9)\n    prediction, aleatoric, epistemic, entropy, mi, variance, error, m_scores, std_scores = inference(model, image, true , N=experi_repeat, tau=0.9)  # if error to mauny values unpack,check inference fuction and set metrics obtained in model.evaluate\n    #prediction, aleatoric, epistemic, entropy, mi, variance, error,m_scores, std_scores = inference(model, image, gt, N=samples, tau=0.01) # \n    # then we get the above metrics which are actualy the operation on all parrallel 10 image(10,128,128,1) to get one image (128,128)\n    # (128,128) is same as (128,128,1)\n    # all above matric are images of size (128,128), obtained by operations on pixels with corresponding 10 images, \n    #it is an operations on the 10 images (10,128,128,1) to get sigle image (128,128,1)\n    # np.squeeze operation remove if there single dimension in the shape. for example; (128,128,1) to (128,128)\n    # prediction= mean of all predicted image and then comprae the values where overall > tau*max(overall). so to get uncertainity in segmentation of prediction. \n\n\n    true     = np.squeeze(true)\n    \n    n = np.random.randint(0,num)\n    fig, ax = plt.subplots(3,3,figsize=(20,10))\n    \n    #_, bin_acc, dice, recall, precision, iou_smooth,iou_loss_core  = score\n    loss,dice_coefficient,iou,precision,recall,FPR,FNR= m_scores\n    #fig.suptitle('Dice: {:.2f}, Recall: {:.2f}, Precision: {:.2f}\\n'.format(dice, recall, precision), y=1.0, fontsize=14)\n    fig.suptitle('Loss: {:.2f}, IoU: {:.2f}\\n'.format(loss, iou), y=1.0, fontsize=14)\n    \n    cax0 = ax[0,0].imshow(np.squeeze(image))\n    plt.colorbar(cax0, ax=ax[0,0])\n    ax[0,0].set_title('Cell image')\n\n    cax1 = ax[0,1].imshow(prediction)\n    plt.colorbar(cax1, ax=ax[0,1])\n    ax[0,1].set_title('Uncertainity in Segmentation prediction')\n\n    cax2 = ax[0,2].imshow(true)\n    plt.colorbar(cax2, ax=ax[0,2])\n    ax[0,2].set_title('Ground truth segmentation')\n    \n    cax3 = ax[1,0].imshow(aleatoric)\n    plt.colorbar(cax3, ax=ax[1,0])\n    ax[1,0].set_title('Aleatoric uncertainty')\n    \n    cax4 = ax[1,1].imshow(epistemic)\n    plt.colorbar(cax4, ax=ax[1,1])\n    ax[1,1].set_title('Epistemic uncertainty')\n\n    cax5 = ax[1,2].imshow(aleatoric+epistemic)\n    plt.colorbar(cax4, ax=ax[1,2])\n    ax[1,2].set_title('Uncertainty (combined)')\n\n    \n    cax6 = ax[2,0].imshow(entropy)\n    plt.colorbar(cax3, ax=ax[2,0])\n    ax[2,0].set_title('Entropy')\n    \n    cax7 = ax[2,1].imshow(mi)\n    plt.colorbar(cax4, ax=ax[2,1])\n    ax[2,1].set_title('Mutual Information')\n\n    cax8 = ax[2,2].imshow(error)\n    plt.colorbar(cax4, ax=ax[2,2])\n    ax[2,2].set_title('Error')\n    for a in ax.flatten(): a.axis('off')\n        \n    #fig.savefig('prediction_uncertainty_{:03d}.png'.format(i), dpi=300)\n    plt.show()\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:04:09.966723Z","iopub.status.idle":"2023-03-30T13:04:09.967442Z","shell.execute_reply.started":"2023-03-30T13:04:09.967173Z","shell.execute_reply":"2023-03-30T13:04:09.967198Z"},"trusted":true},"execution_count":null,"outputs":[]}]}